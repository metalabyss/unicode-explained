1. Введение. Предпосылки для создания Unicode
=============================================

1.1. Понятие кодировки
----------------------
Любая информация в памяти компьютера хранится в битовом представлении (то есть, кодируется последовательностями нулей и единиц). В частности, это относится и к текстовой информации.

Чтобы из последовательности нулей и единиц получались символы, необходимо определить правила для преобразования из первого во второе, и наоборот. Так возникает понятие **кодировка**.

**Кодировка** — это таблица или набор правил, с помощью которых набору бит однозначно сопоставляется символ из определенного множества. Набор бит, представляющий символ в памяти, будем называть **кодом** символа.

В качестве примера кодировки можно привести ASCII. Это широкоизвестная кодировка, которая представлена в виде таблицы из 2⁷ — 128 символов. Каждому символу в ней однозначно соответствует код из семи битов: например, букве “z” соответствует код «1111010», а пробелу — «0100000».

1.2. Проблемы ранее существовавших кодировок
--------------------------------------------
Беда в том, что различных кодировок существует огромное множество: одной ASCII и ещё парочкой кодировок, к сожалению, всё не ограничивается. Поэтому _порой возникает путаница в том, как интерпретировать тот или иной набор бит_.

Эта проблема обострилась в период, когда стал набирать популярность и распространяться Интернет. И в особенности если пользователям необходимо было работать с информацией на другом языке, алфавит которого состоит не из латиницы: как правило, для каждого языка использовалась своя кодировка. А для некоторых языков — и не одна. За примерами далеко ходить не нужно — для русского языка пользовались популярностью целых три: КОИ-8, CP-2151 и CP-866. Но если для родного языка подобрать кодировку было несложно, то для другого, возможно, нужно было помучиться.

Хотя проблему выбора кодировки можно решить перебором, она меркнет на фоне _необходимости использовать в текстовом документе несколько языков_. Например, русский, японский и китайский. Кодировки европейских языков, используемые в то время, по сути, являлись расширением уже упомянутой ASCII: к имеющимся 128 символам добавлялось еще столько же, но уже для кодирования национального алфавита. Но всё же _256 значений — это очень мало, если речь идет о совместном использовании восточных языков с европейскими_.

Да, нельзя не упомянуть о том, что многобайтные кодировки тоже существовали: например, двухбайтная Big5 для китайского языка, которая тем не менее включала в себя еще японскую кану и кириллицу. Но несмотря на то, что Big5 кодирует большой набор символов, это всего лишь очередная кодировка из огромного множества других, несовместимых друг с другом кодировок.

К счастью, появились добровольцы, которые решили привести все проблемы с кодированием и отображением символов к одному знаменателю — так в 1991 году был предложен стандарт кодирования Unicode. Предложила его организация Консорциум Юникода (англ. _Unicode Consortium_).